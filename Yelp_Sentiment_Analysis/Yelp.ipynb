{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79beff70",
   "metadata": {},
   "source": [
    "# Deep Learning Course\n",
    "\n",
    "## Deep Language Learning\n",
    "\n",
    "### Dataset: Yelp review [(Source: Hugging Face)](https://huggingface.co/datasets/Yelp/yelp_review_full)\n",
    "\n",
    "**Implementation of Various Natural Language Processing Models for Text Classification**\n",
    "\n",
    "A series of various NLP methods (Binary Bag of Words, Frequency Bag of Words, TF-IDF, Word Embedding) are implemented for the binary classification of the Yelp Reviews dataset.\n",
    "\n",
    "The dataset is pre-processed to adapt it to our needs, afterwards the various models are implemented.\n",
    "\n",
    "Binary Bag of Words, Frequency Bag of Words, and TF-IDF are implemented with both ngrams = 1 and ngrams = 2, while Word Embedding is implemented both from scratch and using a pre-computed word embedder [(GloVe 6B 50d)](https://nlp.stanford.edu/projects/glove/).\n",
    "\n",
    "In conclusion, the various models are trained and evaluated on a test set to verify their accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7f16cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries used\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from datasets import concatenate_datasets\n",
    "from datasets import load_dataset\n",
    "import tensorflow as tf\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8b570b",
   "metadata": {},
   "source": [
    "The **Yelp Review** dataset contains commercial activity reviews from Yelp (textual reviews) and a valutation (from 1 to 5 stars). \n",
    "\n",
    "The dataset is binarized by considering reviews from 1 to 2 stars as negative, from 4 to 5 stars as positive, and 3-star reviews as neutral (they will not be included).\n",
    "\n",
    "In conclusion we will work on 2 classes.\n",
    "\n",
    "| Label | Review      |\n",
    "|------|---------------|\n",
    "| 0    | Negative       |\n",
    "| 1    | Positive       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079575f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Dimension:\n",
      "Train: 80000\n",
      "Validation: 20000\n",
      "Test: 40000\n"
     ]
    }
   ],
   "source": [
    "# Pre processing\n",
    "\n",
    "yelp = load_dataset(\"Yelp/yelp_review_full\")\n",
    "\n",
    "def binarize(example):\n",
    "    if example[\"label\"] <= 1:\n",
    "        return {\"binary_label\": 0}\n",
    "    elif example[\"label\"] >= 3:\n",
    "        return {\"binary_label\": 1}\n",
    "    else:\n",
    "        return {\"binary_label\": -1}\n",
    "\n",
    "\n",
    "yelp_binary = yelp.map(binarize)\n",
    "\n",
    "yelp_train_full = yelp_binary[\"train\"].filter(lambda example: example[\"binary_label\"] != -1)\n",
    "yelp_test = yelp_binary[\"test\"].filter(lambda example: example[\"binary_label\"] != -1)\n",
    "\n",
    "negative = yelp_train_full.filter(lambda example: example[\"binary_label\"] == 0)\n",
    "positive = yelp_train_full.filter(lambda example: example[\"binary_label\"] == 1)\n",
    "\n",
    "negative_subset = negative.select(range(50000))\n",
    "positive_subset = positive.select(range(50000))\n",
    "\n",
    "yelp_train_val = concatenate_datasets([negative_subset, positive_subset])\n",
    "\n",
    "yelp_train_val = yelp_train_val.shuffle(seed=21)\n",
    "\n",
    "x_train_val = np.array(yelp_train_val[\"text\"])\n",
    "y_train_val = np.array(yelp_train_val[\"binary_label\"])\n",
    "\n",
    "x_test = np.array(yelp_test[\"text\"])\n",
    "y_test = np.array(yelp_test[\"binary_label\"])\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.2, random_state=21)\n",
    "\n",
    "y_train_final = tf.cast(y_train, tf.float32)\n",
    "y_val = tf.cast(y_val, tf.float32)\n",
    "y_test = tf.cast(y_test, tf.float32)\n",
    "\n",
    "batch_size = 32\n",
    "max_tokens = 20000\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "\n",
    "print(\"Final Dimension:\")\n",
    "print(f\"Train: {len(x_train)}\")\n",
    "print(f\"Validation: {len(x_val)}\")\n",
    "print(f\"Test: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "918e1bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"BINARY_BOW\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " TextVectorization_Binary_BO  (None, 20000)            0         \n",
      " W (TextVectorization)                                           \n",
      "                                                                 \n",
      " Dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " Dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " Output (Dense)              (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Binary Bags of Words (ngrams 1)\n",
    "\n",
    "BBoW_text_vectorization = tf.keras.layers.TextVectorization(name=\"TextVectorization_Binary_BOW\",\n",
    "                                                            max_tokens=max_tokens,\n",
    "                                                            output_mode=\"multi_hot\")\n",
    "\n",
    "BBoW_text_vectorization.adapt(x_train)\n",
    "\n",
    "BBoW_model = tf.keras.Sequential(name=\"BINARY_BOW\", layers=[\n",
    "    tf.keras.Input(shape=(1,), dtype=tf.string),\n",
    "    \n",
    "    BBoW_text_vectorization,\n",
    "    \n",
    "    tf.keras.layers.Dense(name=\"Dense\",\n",
    "                          units=16, \n",
    "                          activation=\"relu\"),\n",
    "    \n",
    "    tf.keras.layers.Dropout(name=\"Dropout\",\n",
    "                            rate=0.5),\n",
    "    \n",
    "    tf.keras.layers.Dense(name=\"Output\",\n",
    "                          units=1, \n",
    "                          activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "optimizer_BBoW = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "BBoW_model.compile(optimizer=optimizer_BBoW,\n",
    "                   loss=\"binary_crossentropy\",\n",
    "                   metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "BBoW_model.summary()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad289b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"BINARY_BOW_ngrams2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " TextVectorization_Binary_BO  (None, 20000)            0         \n",
      " W (TextVectorization)                                           \n",
      "                                                                 \n",
      " Dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " Dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " Output (Dense)              (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Binary Bags of Words (ngrams 2)\n",
    "\n",
    "BBoW2_text_vectorization = tf.keras.layers.TextVectorization(name=\"TextVectorization_Binary_BOW\",\n",
    "                                                             max_tokens=max_tokens,\n",
    "                                                             output_mode=\"multi_hot\",\n",
    "                                                             ngrams=2)\n",
    "\n",
    "BBoW2_text_vectorization.adapt(x_train)\n",
    "\n",
    "BBoW2_model = tf.keras.Sequential(name=\"BINARY_BOW_ngrams2\", layers=[\n",
    "    tf.keras.Input(shape=(1,), dtype=tf.string),\n",
    "    \n",
    "    BBoW2_text_vectorization,\n",
    "    \n",
    "    tf.keras.layers.Dense(name=\"Dense\",\n",
    "                          units=16, \n",
    "                          activation=\"relu\"),\n",
    "    \n",
    "    tf.keras.layers.Dropout(name=\"Dropout\",\n",
    "                            rate=0.5),\n",
    "    \n",
    "    tf.keras.layers.Dense(name=\"Output\",\n",
    "                          units=1, \n",
    "                          activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "optimizer_BBoW2 = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "\n",
    "BBoW2_model.compile(optimizer=optimizer_BBoW2,\n",
    "                    loss=\"binary_crossentropy\",\n",
    "                    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "BBoW2_model.summary()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "241b4db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"FREQUENCY_BOW\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " TextVectorization_Frequency  (None, 20000)            0         \n",
      " _BOW (TextVectorization)                                        \n",
      "                                                                 \n",
      " Normalization (BatchNormali  (None, 20000)            80000     \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " Dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " Dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " Output (Dense)              (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 400,033\n",
      "Trainable params: 360,033\n",
      "Non-trainable params: 40,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Frequency Bag of Words (ngrams 1)\n",
    "\n",
    "FBoW_text_vectorization = tf.keras.layers.TextVectorization(name=\"TextVectorization_Frequency_BOW\",\n",
    "                                                            max_tokens=max_tokens,\n",
    "                                                            output_mode=\"count\")\n",
    "\n",
    "FBoW_text_vectorization.adapt(x_train)\n",
    "\n",
    "FBoW_model = tf.keras.Sequential(name=\"FREQUENCY_BOW\", layers=[\n",
    "    tf.keras.Input(shape=(1,), dtype=tf.string),\n",
    "    \n",
    "    FBoW_text_vectorization,\n",
    "    \n",
    "    tf.keras.layers.BatchNormalization(name=\"Normalization\"),\n",
    "    \n",
    "    tf.keras.layers.Dense(name=\"Dense\",\n",
    "                          units=16, \n",
    "                          activation=\"relu\"),\n",
    "    \n",
    "    tf.keras.layers.Dropout(name=\"Dropout\",\n",
    "                            rate=0.5),\n",
    "    \n",
    "    tf.keras.layers.Dense(name=\"Output\",\n",
    "                          units=1, \n",
    "                          activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "optimizer_FboW = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "\n",
    "FBoW_model.compile(\n",
    "    optimizer=optimizer_FboW,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "FBoW_model.summary()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "758f9dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"FREQUENCY_BOW_ngrams2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " TextVectorization_Frequency  (None, 20000)            0         \n",
      " _BOW (TextVectorization)                                        \n",
      "                                                                 \n",
      " Normalization (BatchNormali  (None, 20000)            80000     \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " Dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " Dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " Output (Dense)              (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 400,033\n",
      "Trainable params: 360,033\n",
      "Non-trainable params: 40,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Frequency Bag of Words (ngrams 2)\n",
    "\n",
    "FBoW2_text_vectorization = tf.keras.layers.TextVectorization(name=\"TextVectorization_Frequency_BOW\",\n",
    "                                                             max_tokens=max_tokens,\n",
    "                                                             output_mode=\"count\",\n",
    "                                                             ngrams=2)\n",
    "\n",
    "FBoW2_text_vectorization.adapt(x_train)\n",
    "\n",
    "FBoW2_model = tf.keras.Sequential(name=\"FREQUENCY_BOW_ngrams2\", layers=[\n",
    "    tf.keras.Input(shape=(1,), dtype=tf.string),\n",
    "    \n",
    "    FBoW2_text_vectorization,\n",
    "    \n",
    "    tf.keras.layers.BatchNormalization(name=\"Normalization\"),\n",
    "    \n",
    "    tf.keras.layers.Dense(name=\"Dense\",\n",
    "                          units=16, \n",
    "                          activation=\"relu\"),\n",
    "    \n",
    "    tf.keras.layers.Dropout(name=\"Dropout\",\n",
    "                            rate=0.5),\n",
    "    \n",
    "    tf.keras.layers.Dense(name=\"Output\",\n",
    "                          units=1, \n",
    "                          activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "optimizer_FBoW2 = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "\n",
    "FBoW2_model.compile(\n",
    "    optimizer=optimizer_FBoW2,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "FBoW2_model.summary()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ff0824f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"TF_IDF\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " TextVectorization_TF_IDF (T  (None, 20000)            1         \n",
      " extVectorization)                                               \n",
      "                                                                 \n",
      " Normalization (BatchNormali  (None, 20000)            80000     \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " Dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " Dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " Output (Dense)              (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 400,034\n",
      "Trainable params: 360,033\n",
      "Non-trainable params: 40,001\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF (ngrams 1)\n",
    "\n",
    "TFIDF_text_vectorization = tf.keras.layers.TextVectorization(name=\"TextVectorization_TF_IDF\",\n",
    "                                                             max_tokens=max_tokens,\n",
    "                                                             output_mode=\"tf_idf\")\n",
    "\n",
    "TFIDF_text_vectorization.adapt(x_train)\n",
    "\n",
    "TFIDF_model = tf.keras.Sequential(name=\"TF_IDF\", layers=[\n",
    "    tf.keras.Input(shape=(1,), dtype=tf.string),\n",
    "    \n",
    "    TFIDF_text_vectorization,\n",
    "    \n",
    "    tf.keras.layers.BatchNormalization(name=\"Normalization\"),\n",
    "    \n",
    "    tf.keras.layers.Dense(name=\"Dense\",\n",
    "                          units=16, \n",
    "                          activation=\"relu\"),\n",
    "    \n",
    "    tf.keras.layers.Dropout(name=\"Dropout\",\n",
    "                            rate=0.5),\n",
    "    \n",
    "    tf.keras.layers.Dense(name=\"Output\",\n",
    "                          units=1, \n",
    "                          activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "optimizer_TFIDF = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "\n",
    "TFIDF_model.compile(\n",
    "    optimizer=optimizer_TFIDF,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "TFIDF_model.summary()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de25dc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"TF_IDF_ngrams2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " TextVectorization_TF_IDF (T  (None, 20000)            1         \n",
      " extVectorization)                                               \n",
      "                                                                 \n",
      " Normalization (BatchNormali  (None, 20000)            80000     \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " Dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " Dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " Output (Dense)              (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 400,034\n",
      "Trainable params: 360,033\n",
      "Non-trainable params: 40,001\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF (ngrams 2)\n",
    "\n",
    "TFIDF2_text_vectorization = tf.keras.layers.TextVectorization(name=\"TextVectorization_TF_IDF\",\n",
    "                                                              max_tokens=max_tokens,\n",
    "                                                              output_mode=\"tf_idf\",\n",
    "                                                              ngrams=2)\n",
    "\n",
    "TFIDF2_text_vectorization.adapt(x_train)\n",
    "\n",
    "TFIDF2_model = tf.keras.Sequential(name=\"TF_IDF_ngrams2\", layers=[\n",
    "    tf.keras.Input(shape=(1,), dtype=tf.string),\n",
    "    \n",
    "    TFIDF2_text_vectorization,\n",
    "    \n",
    "    tf.keras.layers.BatchNormalization(name=\"Normalization\"),\n",
    "    \n",
    "    tf.keras.layers.Dense(name=\"Dense\",\n",
    "                          units=16, \n",
    "                          activation=\"relu\"),\n",
    "    \n",
    "    tf.keras.layers.Dropout(name=\"Dropout\",\n",
    "                            rate=0.5),\n",
    "    \n",
    "    tf.keras.layers.Dense(name=\"Output\",\n",
    "                          units=1, \n",
    "                          activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "optimizer_TFIDF2 = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "\n",
    "TFIDF2_model.compile(\n",
    "    optimizer=optimizer_TFIDF2,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "TFIDF2_model.summary()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8747570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Word_Embedding\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Embedding (Embedding)       (None, 350, 128)          2560000   \n",
      "                                                                 \n",
      " LSTM (Bidirectional)        (None, 64)                41216     \n",
      "                                                                 \n",
      " Dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " Output (Dense)              (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,601,281\n",
      "Trainable params: 2,601,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_length = 350  # 95 percentile\n",
    "\n",
    "WordEmbedding_text_vectorization = tf.keras.layers.TextVectorization(max_tokens=max_tokens,\n",
    "                                                                     output_sequence_length=max_length,\n",
    "                                                                     output_mode=\"int\")\n",
    "\n",
    "WordEmbedding_text_vectorization.adapt(x_train)\n",
    "\n",
    "preprocessed_train_ds = train_ds.map(lambda x, y: (WordEmbedding_text_vectorization(x), y))\n",
    "\n",
    "preprocessed_val_ds = val_ds.map(lambda x, y: (WordEmbedding_text_vectorization(x), y))\n",
    "\n",
    "preprocessed_test_ds = test_ds.map(lambda x, y: (WordEmbedding_text_vectorization(x), y))\n",
    "\n",
    "WordEmbedding_model = tf.keras.Sequential(name=\"Word_Embedding\", layers=[\n",
    "    tf.keras.Input(shape=(max_length,), \n",
    "                   dtype=tf.int64),\n",
    "    \n",
    "    tf.keras.layers.Embedding(name=\"Embedding\",\n",
    "                              input_dim=max_tokens,\n",
    "                              output_dim=128),\n",
    "    \n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=32),\n",
    "                                  name=\"LSTM\"),\n",
    "    \n",
    "    tf.keras.layers.Dropout(name=\"Dropout\",\n",
    "                            rate=0.5),\n",
    "    \n",
    "    tf.keras.layers.Dense(name=\"Output\",\n",
    "                          units=1, \n",
    "                          activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "optimizer_WE = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "\n",
    "WordEmbedding_model.compile(\n",
    "    optimizer=optimizer_WE,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "WordEmbedding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7547fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"glove.6B.50d.txt\"):\n",
    "    print(\"Downloading GloVe 6B 50d...\")\n",
    "    url = \"https://nlp.stanford.edu/data/glove.6B.50d.txt\"\n",
    "    urllib.request.urlretrieve(url, \"glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf13f074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Word_Embedding_GloVe\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Embedding (Embedding)       (None, 350, 50)           1000000   \n",
      "                                                                 \n",
      " LSTM (Bidirectional)        (None, 64)                21248     \n",
      "                                                                 \n",
      " Dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " Output (Dense)              (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,021,313\n",
      "Trainable params: 21,313\n",
      "Non-trainable params: 1,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_length = 350  # 95 percentile\n",
    "\n",
    "WordEmbedding_GloVe_text_vectorization = tf.keras.layers.TextVectorization(max_tokens=max_tokens,\n",
    "                                                                           output_sequence_length=max_length,\n",
    "                                                                           output_mode=\"int\")\n",
    "\n",
    "WordEmbedding_GloVe_text_vectorization.adapt(x_train)\n",
    "\n",
    "vocab = WordEmbedding_GloVe_text_vectorization.get_vocabulary()\n",
    "word_index = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "glove_file = \"glove.6B.50d.txt\"  \n",
    "\n",
    "embeddings_index = {}\n",
    "with open(glove_file, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embedding_dim = 50  \n",
    "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < max_tokens:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "preprocessed_train_ds = train_ds.map(lambda x, y: (WordEmbedding_GloVe_text_vectorization(x), y))\n",
    "\n",
    "preprocessed_val_ds = val_ds.map(lambda x, y: (WordEmbedding_GloVe_text_vectorization(x), y))\n",
    "\n",
    "preprocessed_test_ds = test_ds.map(lambda x, y: (WordEmbedding_GloVe_text_vectorization(x), y))\n",
    "\n",
    "WordEmbedding_GloVe_model = tf.keras.Sequential(name=\"Word_Embedding_GloVe\", layers=[\n",
    "    tf.keras.Input(shape=(max_length,), \n",
    "                   dtype=tf.int64),\n",
    "    \n",
    "    tf.keras.layers.Embedding(name=\"Embedding\",\n",
    "                              input_dim=max_tokens,\n",
    "                              output_dim=embedding_dim,\n",
    "                              embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "                              trainable=False,\n",
    "                              mask_zero=True),\n",
    "    \n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=32),\n",
    "                                  name=\"LSTM\"),\n",
    "    \n",
    "    tf.keras.layers.Dropout(name=\"Dropout\",\n",
    "                            rate=0.5),\n",
    "    \n",
    "    tf.keras.layers.Dense(name=\"Output\",\n",
    "                          units=1, \n",
    "                          activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "optimizer_WEGloVe = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "\n",
    "WordEmbedding_GloVe_model.compile(\n",
    "    optimizer=optimizer_WEGloVe,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "WordEmbedding_GloVe_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8fd5357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBoW fit:\n",
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 36s 14ms/step - loss: 0.2716 - accuracy: 0.8954 - val_loss: 0.1992 - val_accuracy: 0.9207\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 35s 14ms/step - loss: 0.1910 - accuracy: 0.9276 - val_loss: 0.2020 - val_accuracy: 0.9220\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 33s 13ms/step - loss: 0.1650 - accuracy: 0.9365 - val_loss: 0.2150 - val_accuracy: 0.9216\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 35s 14ms/step - loss: 0.1476 - accuracy: 0.9445 - val_loss: 0.2215 - val_accuracy: 0.9218\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.2057 - accuracy: 0.9212\n",
      "BBoW2 fit:\n",
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 32s 13ms/step - loss: 0.2439 - accuracy: 0.9102 - val_loss: 0.1729 - val_accuracy: 0.9341\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 32s 13ms/step - loss: 0.1539 - accuracy: 0.9447 - val_loss: 0.1761 - val_accuracy: 0.9351\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 32s 13ms/step - loss: 0.1246 - accuracy: 0.9545 - val_loss: 0.1899 - val_accuracy: 0.9354\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 30s 12ms/step - loss: 0.1055 - accuracy: 0.9623 - val_loss: 0.2014 - val_accuracy: 0.9338\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 31s 12ms/step - loss: 0.0930 - accuracy: 0.9661 - val_loss: 0.2254 - val_accuracy: 0.9343\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 0.1838 - accuracy: 0.9341\n",
      "FBoW fit:\n",
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 42s 16ms/step - loss: 0.3153 - accuracy: 0.8685 - val_loss: 0.3417 - val_accuracy: 0.8924\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 39s 16ms/step - loss: 0.2471 - accuracy: 0.9005 - val_loss: 0.3677 - val_accuracy: 0.8940\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 41s 16ms/step - loss: 0.2197 - accuracy: 0.9097 - val_loss: 0.4349 - val_accuracy: 0.8863\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 38s 15ms/step - loss: 0.2019 - accuracy: 0.9155 - val_loss: 0.5286 - val_accuracy: 0.8789\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.4109 - accuracy: 0.8899\n",
      "FBoW2 fit:\n",
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 43s 17ms/step - loss: 0.2984 - accuracy: 0.8777 - val_loss: 0.3099 - val_accuracy: 0.8967\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 44s 18ms/step - loss: 0.2159 - accuracy: 0.9159 - val_loss: 0.3555 - val_accuracy: 0.8887\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 44s 18ms/step - loss: 0.1804 - accuracy: 0.9297 - val_loss: 0.4499 - val_accuracy: 0.8706\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 0.3475 - accuracy: 0.8941\n",
      "TFIDF fit:\n",
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 44s 17ms/step - loss: 0.3211 - accuracy: 0.8659 - val_loss: 0.7439 - val_accuracy: 0.8407\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 43s 17ms/step - loss: 0.2536 - accuracy: 0.8984 - val_loss: 0.7905 - val_accuracy: 0.8428\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 44s 18ms/step - loss: 0.2253 - accuracy: 0.9107 - val_loss: 0.8933 - val_accuracy: 0.8191\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 42s 17ms/step - loss: 0.2087 - accuracy: 0.9169 - val_loss: 1.1219 - val_accuracy: 0.8210\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 1.0670 - accuracy: 0.8341\n",
      "TFIDF2 fit:\n",
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 46s 18ms/step - loss: 0.2901 - accuracy: 0.8796 - val_loss: 0.3409 - val_accuracy: 0.9024\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 43s 17ms/step - loss: 0.2068 - accuracy: 0.9163 - val_loss: 0.3854 - val_accuracy: 0.8968\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 45s 18ms/step - loss: 0.1733 - accuracy: 0.9305 - val_loss: 0.4877 - val_accuracy: 0.8813\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 0.3539 - accuracy: 0.9017\n",
      "WordEmbedding fit:\n",
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 451s 178ms/step - loss: 0.3199 - accuracy: 0.8705 - val_loss: 0.2192 - val_accuracy: 0.9153\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 436s 175ms/step - loss: 0.1819 - accuracy: 0.9355 - val_loss: 0.2085 - val_accuracy: 0.9248\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 430s 172ms/step - loss: 0.2019 - accuracy: 0.9272 - val_loss: 0.2373 - val_accuracy: 0.9200\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 420s 168ms/step - loss: 0.1258 - accuracy: 0.9568 - val_loss: 0.2094 - val_accuracy: 0.9292\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 346s 138ms/step - loss: 0.0925 - accuracy: 0.9694 - val_loss: 0.2394 - val_accuracy: 0.9243\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 254s 102ms/step - loss: 0.0717 - accuracy: 0.9774 - val_loss: 0.3910 - val_accuracy: 0.8962\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 0.2271 - accuracy: 0.9246\n",
      "WordEmbedding_GloVe fit:\n",
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 232s 91ms/step - loss: 0.4109 - accuracy: 0.8123 - val_loss: 0.3148 - val_accuracy: 0.8676\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 226s 90ms/step - loss: 0.3132 - accuracy: 0.8678 - val_loss: 0.3049 - val_accuracy: 0.8709\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 227s 91ms/step - loss: 0.2772 - accuracy: 0.8850 - val_loss: 0.2802 - val_accuracy: 0.8846\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 229s 92ms/step - loss: 0.2562 - accuracy: 0.8958 - val_loss: 0.2617 - val_accuracy: 0.8915\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 231s 92ms/step - loss: 0.2378 - accuracy: 0.9036 - val_loss: 0.2493 - val_accuracy: 0.8987\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 233s 93ms/step - loss: 0.2244 - accuracy: 0.9096 - val_loss: 0.2348 - val_accuracy: 0.9042\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 232s 93ms/step - loss: 0.2141 - accuracy: 0.9146 - val_loss: 0.2270 - val_accuracy: 0.9099\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 233s 93ms/step - loss: 0.2038 - accuracy: 0.9189 - val_loss: 0.2230 - val_accuracy: 0.9115\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 233s 93ms/step - loss: 0.1955 - accuracy: 0.9232 - val_loss: 0.2331 - val_accuracy: 0.9075\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 233s 93ms/step - loss: 0.1887 - accuracy: 0.9261 - val_loss: 0.2335 - val_accuracy: 0.9081\n",
      "1250/1250 [==============================] - 30s 24ms/step - loss: 0.2266 - accuracy: 0.9117\n"
     ]
    }
   ],
   "source": [
    "# Fitting and Evaluation\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", \n",
    "                                                  patience = 2,\n",
    "                                                  restore_best_weights=True)\n",
    "\n",
    "models = [\"BBoW\", \"BBoW2\", \"FBoW\", \"FBoW2\", \"TFIDF\", \"TFIDF2\", \"WordEmbedding\", \"WordEmbedding_GloVe\"]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name in models:\n",
    "    if model_name == \"BBoW\":\n",
    "        model = BBoW_model\n",
    "        train = train_ds\n",
    "        val = val_ds\n",
    "        test = test_ds\n",
    "    elif model_name == \"BBoW2\":\n",
    "        model = BBoW2_model\n",
    "        train = train_ds\n",
    "        val = val_ds\n",
    "        test = test_ds\n",
    "    elif model_name == \"FBoW\":\n",
    "        model = FBoW_model\n",
    "        train = train_ds\n",
    "        val = val_ds\n",
    "        test = test_ds\n",
    "    elif model_name == \"FBoW2\":\n",
    "        model = FBoW2_model\n",
    "        train = train_ds\n",
    "        val = val_ds\n",
    "        test = test_ds\n",
    "    elif model_name == \"TFIDF\":\n",
    "        model = TFIDF_model\n",
    "        train = train_ds\n",
    "        val = val_ds\n",
    "        test = test_ds\n",
    "    elif model_name == \"TFIDF2\":\n",
    "        model = TFIDF2_model\n",
    "        train = train_ds\n",
    "        val = val_ds\n",
    "        test = test_ds\n",
    "    elif model_name == \"WordEmbedding\":\n",
    "        model = WordEmbedding_model\n",
    "        train = preprocessed_train_ds\n",
    "        val = preprocessed_val_ds\n",
    "        test = preprocessed_test_ds\n",
    "    elif model_name == \"WordEmbedding_GloVe\":\n",
    "        model = WordEmbedding_GloVe_model\n",
    "        train = preprocessed_train_ds\n",
    "        val = preprocessed_val_ds\n",
    "        test = preprocessed_test_ds\n",
    "        \n",
    "    print(f\"{model_name} fit:\")\n",
    "    model.fit(train,\n",
    "              epochs=10,\n",
    "              validation_data=val,\n",
    "              callbacks=[early_stopping])\n",
    "    \n",
    "    test_loss, test_accuracy = model.evaluate(test)\n",
    "    \n",
    "    results[model_name] = (test_loss, test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61e18079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Model         | Test Loss  | Test Accuracy\n",
      "----------------------------------------------\n",
      "       BBoW2         |   0.1838   |   0.9341  \n",
      "   WordEmbedding     |   0.2271   |   0.9246  \n",
      "        BBoW         |   0.2057   |   0.9212  \n",
      "WordEmbedding_GloVe  |   0.2266   |   0.9117  \n",
      "       TFIDF2        |   0.3539   |   0.9017  \n",
      "       FBoW2         |   0.3475   |   0.8941  \n",
      "        FBoW         |   0.4109   |   0.8899  \n",
      "       TFIDF         |   1.0670   |   0.8341  \n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "\n",
    "print(f\"{'Model':^20} | {'Test Loss':^10} | {'Test Accuracy':^10}\")\n",
    "print(\"-\" * 46)  \n",
    "\n",
    "sorted_models = sorted(models, \n",
    "                       key=lambda x: results[x][1], \n",
    "                       reverse=True)\n",
    "\n",
    "for model in sorted_models:\n",
    "    print(f\"{model:^20} | {results[model][0]:^10.4f} | {results[model][1]:^10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05cb307",
   "metadata": {},
   "source": [
    "It can be noted that setting ngrams=2 improves the model in all cases (Binary Bag of Words, Frequency Bag of Words, and TFIDF).\n",
    "\n",
    "Among Word Embeddings, there is a subtle difference in accuracy in favor of the model implemented from scratch.\n",
    "\n",
    "The TFIDF model (ngrams=1) is the worst with a gap of 10 percentage points in accuracy compared to the best (Binary Bag of Words with ngrams=2), and it also has a high loss.\n",
    "\n",
    "In general, excluding TFIDF with ngrams=1, all models have an acceptable accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
